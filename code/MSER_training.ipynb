{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy.io as sio\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, M, N, K, snr = 5, 10, 1, 10, 5\n",
    "layer, batch_size, I_max = 3, 20, 200\n",
    "p_ = tf.random_normal(mean=0.0,stddev=tf.sqrt(0.5),shape=[2*K*M, N])\n",
    "p = tf.complex(p_[0:K*M,:], p_[K*M:,:])\n",
    "w_ = tf.random_normal(mean=0.0,stddev=tf.sqrt(0.5),shape=[2*K*L, N])\n",
    "w = tf.complex(w_[0:K*L,:], w_[K*L:,:])\n",
    "n_ = tf.random_normal(mean=0.0,stddev=tf.sqrt(tf.sqrt(0.1)*0.5),shape=[2*L, 1])\n",
    "n = tf.complex(n_[0:L,:], n_[L:,:])\n",
    "b = tf.placeholder(dtype=tf.complex64, shape=[K,batch_size])\n",
    "H_ = tf.placeholder(dtype=tf.float32, shape=[2*K*L, M])\n",
    "H = tf.complex(H_[0:K*L,:], H_[K*L:,:])\n",
    "lr_p = tf.complex(tf.random_normal(mean=0.0,stddev=tf.sqrt(0.1),shape=[K*M, N]), tf.zeros(shape=[K*M,N]))\n",
    "lr_w = tf.complex(tf.random_normal(mean=0.0,stddev=tf.sqrt(0.1),shape=[K*L, N]), tf.zeros(shape=[K*L,N]))\n",
    "\n",
    "error = []\n",
    "train_b = tf.sign(tf.random_uniform(shape=[10000, K], minval=-1,maxval=1, dtype=tf.float32))\n",
    "test_b = tf.sign(tf.random_uniform(shape=[3000, K], minval=-1,maxval=1, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(10, 20), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "sum2 = tf.complex(tf.zeros(shape=[M,M]), tf.zeros(shape=[M,M]))\n",
    "rol = tf.complex(tf.zeros(shape=[K,1]), tf.zeros(shape=[K,1]))\n",
    "for k in range(0,K):\n",
    "    sum2 = sum2+tf.matmul(p[k*M:(k+1)*M,:], tf.transpose(tf.conj(p[k*M:(k+1)*M,:])))\n",
    "for k in range(0,K):\n",
    "    part0 = rol[0:k,:]\n",
    "    part1 = tf.sqrt(tf.matmul(tf.matmul(tf.matmul(tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), H[k*L:(k+1)*L,:]), sum2),\n",
    "                                        tf.transpose(tf.conj(H[k*L:(k+1)*L,:]))), w[k*L:(k+1)*L,:]))\n",
    "    part2 = rol[k+1:,:]\n",
    "    rol = tf.concat([part0,part1,part2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(p,w,lr_p,lr_w):\n",
    "    b_hat = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "    delta_w = tf.complex(tf.zeros(shape=[K*L,batch_size]), tf.zeros(shape=[K*L,batch_size]))\n",
    "    delta_p = tf.complex(tf.zeros(shape=[K*M,batch_size]), tf.zeros(shape=[K*M,batch_size]))\n",
    "    delta_p_avr = tf.complex(tf.zeros(shape=[K*M,1]), tf.zeros(shape=[K*M,1]))\n",
    "    delta_w_avr = tf.complex(tf.zeros(shape=[K*L,1]), tf.zeros(shape=[K*L,1]))\n",
    "    \n",
    "    sum1 = tf.complex(tf.zeros(shape=[M,batch_size]), tf.zeros(shape=[M,batch_size]))\n",
    "    part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "    for i in range(0,batch_size):\n",
    "        part0 = sum1[:,0:i]\n",
    "        part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        part2 = sum1[:,(i+1):]\n",
    "        for k in range(0,K):\n",
    "            part1 = part1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "        sum1 = tf.concat([part0,part1,part2], axis=1)\n",
    "    #print(sum1)\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = b_hat[:k,:]\n",
    "            part1 = b_hat[k:(k+1),:i]\n",
    "            part2 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n))\n",
    "            part3 = b_hat[k:(k+1),(i+1):]\n",
    "            part4 = b_hat[(k+1):,:]\n",
    "            b_hat = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = delta_p[:k*M,:]\n",
    "            part1 = delta_p[k*M:(k+1)*M,:i]\n",
    "            part2 = -(b[k:(k+1),i:(i+1)]/(2*tf.sqrt(2*3.14*rol[k:(k+1),:]))\n",
    "                      *tf.exp(-tf.complex(tf.pow(tf.real(b_hat[k:(k+1),i:(i+1)]),2), tf.zeros(shape=[1,1]))/(2*tf.pow(rol[k:(k+1),:],2)))\n",
    "                      *b[k:(k+1),i:(i+1)]*tf.matmul(tf.transpose(tf.conj(H[k*L:(k+1)*L,:])), w[k*L:(k+1)*L,:]))\n",
    "            part3 = delta_p[k*M:(k+1)*M,(i+1):]\n",
    "            part4 = delta_p[(k+1)*M:,:]\n",
    "            delta_p = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    delta_p_avr = tf.reduce_mean(delta_p, axis=1, keep_dims=True)\n",
    "    p = p-lr_p*delta_p_avr\n",
    "    \n",
    "    sum1 = tf.complex(tf.zeros(shape=[M,batch_size]), tf.zeros(shape=[M,batch_size]))\n",
    "    part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "    for i in range(0,batch_size):\n",
    "        part0 = sum1[:,0:i]\n",
    "        part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        part2 = sum1[:,(i+1):]\n",
    "        for k in range(0,K):\n",
    "            part1 = part1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "        sum1 = tf.concat([part0,part1,part2], axis=1)\n",
    "    #print(sum1)\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = b_hat[:k,:]\n",
    "            part1 = b_hat[k:(k+1),:i]\n",
    "            part2 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n))\n",
    "            part3 = b_hat[k:(k+1),(i+1):]\n",
    "            part4 = b_hat[(k+1):,:]\n",
    "            b_hat = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = delta_w[:k*L,:]\n",
    "            part1 = delta_w[k*L:(k+1)*L,:i]\n",
    "            part2 = -(b[k:(k+1),i:(i+1)]/(2*tf.sqrt(2*3.14*rol[k:(k+1),:]))\n",
    "                      *tf.exp(-tf.complex(tf.pow(tf.real(b_hat[k:(k+1),i:(i+1)]),2), tf.zeros(shape=[1,1]))/(2*tf.pow(rol[k:(k+1),:],2)))\n",
    "                      *tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]))\n",
    "            part3 = delta_w[k*L:(k+1)*L,(i+1):]\n",
    "            part4 = delta_w[(k+1)*L:,:]\n",
    "            delta_w = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    delta_w_avr = tf.reduce_mean(delta_w, axis=1, keep_dims=True)\n",
    "    w = w-lr_w*delta_w_avr\n",
    "    return p,w,delta_p_avr,delta_w_avr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back(p,w, G_p,G_w):\n",
    "    #sum1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "    #b_hat = tf.complex(tf.zeros(shape=[K,1]), tf.zeros(shape=[K,1]))\n",
    "    #for k in range(0,K):\n",
    "    #    sum1 = sum1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "    \n",
    "    sum1 = tf.complex(tf.zeros(shape=[M,batch_size]), tf.zeros(shape=[M,batch_size]))\n",
    "    part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "    for i in range(0,batch_size):\n",
    "        part0 = sum1[:,0:i]\n",
    "        part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        part2 = sum1[:,(i+1):]\n",
    "        for k in range(0,K):\n",
    "            part1 = part1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "        sum1 = tf.concat([part0,part1,part2], axis=1)\n",
    "    \n",
    "    #for k in range(0,K):\n",
    "    #    part0 = b_hat[:k,:]\n",
    "    #    part1 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1), n))\n",
    "    #    part2 = b_hat[(k+1):,:]\n",
    "    #    b_hat = tf.concat([part0,part1,part2], axis=0)\n",
    "    \n",
    "    b_hat = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = b_hat[:k,:]\n",
    "            part1 = b_hat[k:(k+1),:i]\n",
    "            part2 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n))\n",
    "            part3 = b_hat[k:(k+1),(i+1):]\n",
    "            part4 = b_hat[(k+1):,:]\n",
    "            b_hat = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    \n",
    "    cof1 = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "    cof2 = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "    for i in range(0,batch_size):\n",
    "        part0 = cof1[:,:i]\n",
    "        part1 = (b[:,i:(i+1)]/(2*tf.sqrt(2*3.14*rol))\n",
    "                 *tf.exp(-tf.complex(tf.pow(tf.real(b_hat[:,i:(i+1)]),2), tf.zeros(shape=[K,1]))/(2*tf.pow(rol,2))))\n",
    "        part2 = cof1[:,(i+1):]\n",
    "        cof1 = tf.concat([part0,part1,part2], axis=1)\n",
    "        \n",
    "        part0 = cof2[:,:i]\n",
    "        part1 = cof1[:,i:(i+1)]*tf.complex(tf.real(b_hat[:,i:(i+1)]), tf.zeros(shape=[K,1]))/tf.pow(rol,2)\n",
    "        part2 = cof2[:,(i+1):]\n",
    "        cof2 = tf.concat([part0,part1,part2], axis=1)\n",
    "    \n",
    "    \n",
    "    G_p_index = tf.complex(tf.zeros(shape=[batch_size,K*M]), tf.zeros(shape=[batch_size,K*M]))\n",
    "    G_w_index = tf.complex(tf.zeros(shape=[batch_size,K*L]), tf.zeros(shape=[batch_size,K*L]))\n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = G_p_index[:i,:]\n",
    "            part1 = G_p_index[i:(i+1),:k*M]\n",
    "            part2 = (tf.matmul(G_w[:,k*L:(k+1)*L]*tf.transpose(lr_w[k*L:(k+1)*L,:]), tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]))\n",
    "                     *cof2[k:(k+1),i:(i+1)]*b[k:(k+1),i:(i+1)]*tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), H[k*L:(k+1)*L,:])\n",
    "                     +tf.matmul(G_w[:,k*L:(k+1)*L]*tf.transpose(lr_w[k*L:(k+1)*L,:])*cof1[k:(k+1),i:(i+1)]*b[k:(k+1),i:(i+1)], H[k*L:(k+1)*L,:])\n",
    "                     +tf.matmul(G_p[:,k*M:(k+1)*M]*tf.transpose(lr_p[k*M:(k+1)*M,:]), tf.matmul(tf.transpose(tf.conj(H[k*L:(k+1)*L,:])), w[k*L:(k+1)*L,:])\n",
    "                                *b[k:(k+1),i:(i+1)])*cof2[k:(k+1),i:(i+1)]*b[k:(k+1),i:(i+1)]*tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), H[k*L:(k+1)*L,:]))\n",
    "            part3 = G_p_index[i:(i+1),(k+1)*M:]\n",
    "            part4 = G_p_index[(i+1):,:]\n",
    "            G_p_index = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "\n",
    "            part0 = G_w_index[:i,:]\n",
    "            part1 = G_w_index[i:(i+1),:k*L]\n",
    "            part2 = (tf.matmul(G_w[:,k*L:(k+1)*L]*tf.transpose(lr_w[k*L:(k+1)*L,:]), tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]))\n",
    "                     *cof2[k:(k+1),i:(i+1)]*tf.transpose(tf.conj(tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n)))\n",
    "                     +tf.matmul(G_p[:,k*M:(k+1)*M]*tf.transpose(lr_p[k*M:(k+1)*M,:]), tf.matmul(tf.transpose(tf.conj(H[k*L:(k+1)*L,:])), w[k*L:(k+1)*L,:])\n",
    "                                *b[k:(k+1),i:(i+1)])*cof2[k:(k+1),i:(i+1)]*tf.transpose(tf.conj(tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n)))\n",
    "                     +tf.matmul(G_p[:,k*M:(k+1)*M]*tf.transpose(lr_p[k*M:(k+1)*M,:])*cof1[k:(k+1),i:(i+1)]*b[k:(k+1),i:(i+1)], tf.transpose(tf.conj(H[k*L:(k+1)*L,:]))))\n",
    "            part3 = G_w_index[i:(i+1),(k+1)*L:]\n",
    "            part4 = G_w_index[(i+1):,:]\n",
    "            G_w_index = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    \n",
    "    G_p_index = tf.reduce_mean(G_p_index, axis=0, keep_dims=True)\n",
    "    G_w_index = tf.reduce_mean(G_w_index, axis=0, keep_dims=True)\n",
    "        \n",
    "    return G_p_index, G_w_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shishuhan\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "sess = tf.InteractiveSession(config=tf_config)\n",
    "    \n",
    "delta_ap = tf.complex(tf.zeros(shape=[K*M,layer]), tf.zeros(shape=[K*M,layer]))\n",
    "delta_aw = tf.complex(tf.zeros(shape=[K*L,layer]), tf.zeros(shape=[K*L,layer]))\n",
    "for l in range(0,layer):\n",
    "    p, w, delta_p_avr, delta_w_avr = forward(p,w,lr_p,lr_w)\n",
    "    \n",
    "    part0 = delta_ap[:,:l]\n",
    "    part1 = -delta_p_avr\n",
    "    part2 = delta_ap[:,(l+1):]\n",
    "    delta_ap = tf.concat([part0,part1,part2], axis = 1)\n",
    "    \n",
    "    part0 = delta_aw[:,:l]\n",
    "    part1 = -delta_w_avr\n",
    "    part2 = delta_aw[:,(l+1):]\n",
    "    delta_aw = tf.concat([part0,part1,part2], axis = 1)\n",
    "    \n",
    "    sum1 = tf.complex(tf.zeros(shape=[M,batch_size]), tf.zeros(shape=[M,batch_size]))\n",
    "    part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "    for i in range(0,batch_size):\n",
    "        part0 = sum1[:,0:i]\n",
    "        part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        part2 = sum1[:,(i+1):]\n",
    "        for k in range(0,K):\n",
    "            part1 = part1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "        sum1 = tf.concat([part0,part1,part2], axis=1)\n",
    "    #print(sum1)\n",
    "    \n",
    "    b_hat = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            part0 = b_hat[:k,:]\n",
    "            part1 = b_hat[k:(k+1),:i]\n",
    "            part2 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n))\n",
    "            part3 = b_hat[k:(k+1),(i+1):]\n",
    "            part4 = b_hat[(k+1):,:]\n",
    "            b_hat = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "    \n",
    "    s = tf.zeros(shape=[K*N,batch_size])\n",
    "    bit_error = tf.zeros(shape=[1,batch_size])\n",
    "    \n",
    "    def pos(s):\n",
    "        part0 = s[:k,:]\n",
    "        part1 = s[k:(k+1),:i]\n",
    "        part2 = tf.ones(shape=[1,1])\n",
    "        part3 = s[k:(k+1),(i+1):]\n",
    "        part4 = s[(k+1):,:]\n",
    "        s = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0), tf.zeros(shape=[K*N,batch_size])\n",
    "        return s\n",
    "    def neg(s):\n",
    "        part0 = s[:k,:]\n",
    "        part1 = s[k:(k+1),:i]\n",
    "        part2 = -tf.ones(shape=[1,1])\n",
    "        part3 = s[k:(k+1),(i+1):]\n",
    "        part4 = s[(k+1):,:]\n",
    "        s = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0), tf.zeros(shape=[K*N,batch_size])\n",
    "        return s\n",
    "    \n",
    "    def add_error(bit_error):\n",
    "        part0 = bit_error[0:1,:i]\n",
    "        part1 = bit_error[0:1,i:(i+1)]+1\n",
    "        part2 = bit_error[0:1,(i+1):]\n",
    "        bit_error = tf.concat([part0,part1,part2], axis=1)\n",
    "        return bit_error\n",
    "    def remain_error(bit_error):\n",
    "        return bit_error\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        for k in range(0,K):\n",
    "            tf.cond(tf.real(b_hat[k,i]) > 0, lambda: pos(s), lambda: neg(s))\n",
    "            \n",
    "            tf.cond(tf.equal(tf.real(b[k,i]), s[k,i]), lambda: remain_error(bit_error), lambda:add_error(bit_error))\n",
    "                \n",
    "    \n",
    "    error_op = error.append(tf.reduce_mean(bit_error).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_grad(delta_p_avr,delta_w_avr):\n",
    "    G_p = tf.complex(tf.zeros(shape=[layer,K*M]), tf.zeros(shape=[layer,K*M]))\n",
    "    G_w = tf.complex(tf.zeros(shape=[layer,K*L]), tf.zeros(shape=[layer,K*L]))\n",
    "\n",
    "    G_p_L = tf.transpose(tf.conj(delta_p_avr))\n",
    "    G_w_L = tf.transpose(tf.conj(delta_w_avr))\n",
    "    G_p_index = tf.complex(tf.zeros(shape=[1,K*M]), tf.zeros(shape=[1,K*M]))\n",
    "    G_w_index = tf.complex(tf.zeros(shape=[1,K*L]), tf.zeros(shape=[1,K*L]))\n",
    "    #print(G_p_L)\n",
    "\n",
    "    part0 = G_p[:(layer-1),:]\n",
    "    part1 = G_p_L\n",
    "    G_p = tf.concat([part0,part1],axis=0)\n",
    "\n",
    "    part0 = G_w[:(layer-1),:]\n",
    "    part1 = G_w_L\n",
    "    G_w = tf.concat([part0,part1],axis=0)\n",
    "\n",
    "    G_p_index, G_w_index = back(p,w,G_p_index,G_w_index)\n",
    "    part0 = G_p[:(layer-2),:]\n",
    "    part1 = G_p_index\n",
    "    part2 = G_p[(layer-1):,:]\n",
    "    G_p = tf.concat([part0,part1,part2],axis=0)\n",
    "\n",
    "    part0 = G_w[:(layer-2),:]\n",
    "    part1 = G_w_index\n",
    "    part2 = G_w[(layer-1):,:]\n",
    "    G_w = tf.concat([part0,part1,part2],axis=0)\n",
    "\n",
    "    for l in range(layer-3,-1,-1):\n",
    "        G_p_index, G_w_index = back(p,w,G_p_index,G_w_index)\n",
    "        part0 = G_p[:l,:]\n",
    "        part1 = G_p_index\n",
    "        part2 = G_p[(l+1):,:]\n",
    "        G_p = tf.concat([part0,part1,part2],axis=0)\n",
    "\n",
    "        part0 = G_w[:l,:]\n",
    "        part1 = G_w_index\n",
    "        part2 = G_w[(l+1):,:]\n",
    "        G_w = tf.concat([part0,part1,part2],axis=0)\n",
    "\n",
    "    global delta_ap\n",
    "    global delta_aw\n",
    "    delta_ap1 = tf.transpose(tf.conj(G_p))*delta_ap\n",
    "    delta_aw1 = tf.transpose(tf.conj(G_w))*delta_aw\n",
    "\n",
    "    delta_p_layer = tf.transpose(tf.conj(G_p))\n",
    "    delta_w_layer = tf.transpose(tf.conj(G_w))\n",
    "    return delta_ap1,delta_aw1,delta_p_layer,delta_w_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.get_variable('global_step', [], dtype=tf.int32,\n",
    "                              initializer=tf.constant_initializer(0), trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.005, global_step=global_step, decay_steps=50000,\n",
    "                                           decay_rate=0.9, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "    delta_ap,delta_aw,delta_p_layer,delta_w_layer = back_grad(delta_p_avr,delta_w_avr)\n",
    "    \n",
    "    for l in range(0,layer-1):\n",
    "        lr_p = lr_p-tf.complex(learning_rate,tf.zeros(shape=[1,1]))*delta_ap[:,l:(l+1)]\n",
    "        p = p-lr_p*delta_p_layer[:,l:(l+1)]\n",
    "        lr_w = lr_w-tf.complex(learning_rate,tf.zeros(shape=[1,1]))*delta_aw[:,l:(l+1)]\n",
    "        w = w-lr_w*delta_w_layer[:,l:(l+1)]\n",
    "    \n",
    "        sum1 = tf.complex(tf.zeros(shape=[M,batch_size]), tf.zeros(shape=[M,batch_size]))\n",
    "        part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        for i in range(0,batch_size):\n",
    "            part0 = sum1[:,0:i]\n",
    "            part1 = tf.complex(tf.zeros(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "            part2 = sum1[:,(i+1):]\n",
    "            for k in range(0,K):\n",
    "                part1 = part1+tf.matmul(p[k*M:(k+1)*M,:], b[k:(k+1),i:(i+1)])\n",
    "            sum1 = tf.concat([part0,part1,part2], axis=1)\n",
    "        #print(sum1)\n",
    "\n",
    "        b_hat = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "        for i in range(0,batch_size):\n",
    "            for k in range(0,K):\n",
    "                part0 = b_hat[:k,:]\n",
    "                part1 = b_hat[k:(k+1),:i]\n",
    "                part2 = tf.matmul(tf.transpose(tf.conj(w[k*L:(k+1)*L,:])), tf.add(tf.matmul(H[k*L:(k+1)*L,:], sum1[:,i:(i+1)]), n))\n",
    "                part3 = b_hat[k:(k+1),(i+1):]\n",
    "                part4 = b_hat[(k+1):,:]\n",
    "                b_hat = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0)\n",
    "\n",
    "        s = tf.zeros(shape=[K*N,batch_size])\n",
    "        bit_error = tf.zeros(shape=[1,batch_size])\n",
    "\n",
    "        def pos(s):\n",
    "            part0 = s[:k,:]\n",
    "            part1 = s[k:(k+1),:i]\n",
    "            part2 = tf.ones(shape=[1,1])\n",
    "            part3 = s[k:(k+1),(i+1):]\n",
    "            part4 = s[(k+1):,:]\n",
    "            s = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0), tf.zeros(shape=[K*N,batch_size])\n",
    "            return s\n",
    "        def neg(s):\n",
    "            part0 = s[:k,:]\n",
    "            part1 = s[k:(k+1),:i]\n",
    "            part2 = -tf.ones(shape=[1,1])\n",
    "            part3 = s[k:(k+1),(i+1):]\n",
    "            part4 = s[(k+1):,:]\n",
    "            s = tf.concat([part0,tf.concat([part1,part2,part3], axis=1),part4], axis=0), tf.zeros(shape=[K*N,batch_size])\n",
    "            return s\n",
    "\n",
    "        def add_error(bit_error):\n",
    "            part0 = bit_error[0:1,:i]\n",
    "            part1 = bit_error[0:1,i:(i+1)]+1\n",
    "            part2 = bit_error[0:1,(i+1):]\n",
    "            bit_error = tf.concat([part0,part1,part2], axis=1)\n",
    "            return bit_error\n",
    "        def remain_error(bit_error):\n",
    "            return bit_error\n",
    "\n",
    "        for i in range(0,batch_size):\n",
    "            for k in range(0,K):\n",
    "                tf.cond(tf.real(b_hat[k,i]) > 0, lambda: pos(s), lambda: neg(s))\n",
    "\n",
    "                tf.cond(tf.equal(tf.real(b[k,i]), s[k,i]), lambda: remain_error(bit_error), lambda:add_error(bit_error))\n",
    "\n",
    "\n",
    "        error1_op = error.append(tf.reduce_mean(bit_error).eval())\n",
    "    \n",
    "    p, w, delta_p_avr, delta_w_avr = forward(p,w,lr_p,lr_w)\n",
    "    \n",
    "    #part0 = delta_ap[:,:l]\n",
    "    #part1 = -delta_p_avr\n",
    "    #part2 = delta_ap[:,(l+1):]\n",
    "    #delta_ap = tf.concat([part0,part1,part2], axis = 1)\n",
    "    #\n",
    "    #part0 = delta_aw[:,:l]\n",
    "    #part1 = -delta_w_avr\n",
    "    #part2 = delta_aw[:,(l+1):]\n",
    "    #delta_aw = tf.concat([part0,part1,part2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Complex_2063:0\", shape=(10, 20), dtype=complex64)\n",
      "Tensor(\"Complex_2064:0\", shape=(10, 20), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "input_train_queue = tf.train.slice_input_producer([train_b], shuffle=True)\n",
    "train_b_batch = tf.train.batch(input_train_queue, batch_size=batch_size, num_threads=1, capacity=32)\n",
    "train_b_ = tf.complex(tf.transpose(train_b_batch), tf.zeros(shape=[K, batch_size], dtype = tf.float32))\n",
    "print(train_b_)\n",
    "    \n",
    "input_test_queue = tf.train.slice_input_producer([test_b], shuffle=True)\n",
    "test_b_batch = tf.train.batch(input_test_queue, batch_size=batch_size, num_threads=1, capacity=32)\n",
    "test_b_ = tf.complex(tf.transpose(test_b_batch), tf.zeros(shape=[K, batch_size], dtype = tf.float32))\n",
    "print(test_b_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 , train_error:0.000000\n",
      "---------------------\n",
      "step:0 , test_error:0.000000\n",
      "---------------------\n",
      "step:10 , train_error:0.000000\n",
      "step:20 , train_error:0.000000\n",
      "step:30 , train_error:0.000000\n",
      "step:40 , train_error:0.000000\n",
      "step:50 , train_error:0.000000\n",
      "step:60 , train_error:0.000000\n",
      "step:70 , train_error:0.000000\n",
      "step:80 , train_error:0.000000\n",
      "step:90 , train_error:0.000000\n",
      "step:100 , train_error:0.000000\n",
      "---------------------\n",
      "step:100 , test_error:0.000000\n",
      "---------------------\n",
      "step:110 , train_error:0.000000\n",
      "step:120 , train_error:0.000000\n",
      "step:130 , train_error:0.000000\n",
      "step:140 , train_error:0.000000\n",
      "step:150 , train_error:0.000000\n",
      "step:160 , train_error:0.000000\n",
      "step:170 , train_error:0.000000\n",
      "step:180 , train_error:0.000000\n",
      "step:190 , train_error:0.000000\n"
     ]
    }
   ],
   "source": [
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver() \n",
    "    \n",
    "    for i in range(I_max):\n",
    "        #train_loss_, train_acc_, train_batch_, train_batch_ = 0, 0, 0, 0\n",
    "        #train_b_list = shuffle_set(train_b)\n",
    "        for j in range(500):\n",
    "            #train_b_batch = minibatches(inputs=train_b_list, batch_size=batch_size, now_batch=j, total_batch=500)\n",
    "            #train_b_ = tf.complex(tf.transpose(train_b_batch), tf.zeros(shape=[K, batch_size], dtype = tf.float32))\n",
    "            train_batch = sess.run(train_b_)\n",
    "            #_, step, train_loss, train_acc = sess.run([train_op, global_step, loss, accuracy],\n",
    "            #                                          feed_dict={input_b:train_batch, is_training:True})\n",
    "            step = sess.run([global_step], feed_dict={b:train_batch})\n",
    "            \n",
    "            #train_loss_ += train_loss\n",
    "            #train_acc_ += train_acc[1]\n",
    "            #train_batch_ += 1\n",
    "        \n",
    "        #fig_loss1[i] = np.sum(train_loss_)/train_batch_\n",
    "        #fig_acc1[i] = np.sum(train_acc_)/train_batch_\n",
    "            \n",
    "        if i%10 == 0:  # print training \n",
    " #           print(device_lib.list_local_devices())\n",
    "            print('step:%d , train_error:%.6f' % (i, error[-1]))\n",
    "        if i%50 == 0:  # save current model\n",
    "            #save_path = os.path.join('ckpt-mmse-training', 'mser-model.ckpt')\n",
    "            #saver.save(sess, save_path, global_step=step)\n",
    "            sio.savemat(\"training_bit_error.mat\", {\"array\": error})\n",
    "        #if i%100 == 0: # testing\n",
    "            #if tf.train.latest_checkpoint('ckpt-mmse-training') is not None:\n",
    "            #    saver.restore(sess, tf.train.latest_checkpoint('ckpt-mmse-training'))\n",
    "            #test_loss_, test_acc_, test_batch_, test_batch_ = 0, 0, 0, 0\n",
    "            #for j in range(150):\n",
    "                #test_b_list = shuffle_set(test_b)\n",
    "                #test_b_batch = minibatches(inputs=test_b_list, batch_size=batch_size, now_batch=j, total_batch=150)\n",
    "                #test_b_ = tf.complex(tf.transpose(test_b_batch), tf.zeros(shape=[K, batch_size], dtype = tf.float32))\n",
    "                #test_batch = sess.run(test_b_)\n",
    "                #step = sess.run([global_step], feed_dict={b:test_batch})\n",
    "                #step, test_loss, test_acc = sess.run([global_step, loss, accuracy],feed_dict={input_b:test_batch, is_training:False})\n",
    "                #test_loss_ += test_loss\n",
    "                #test_acc_ += test_acc[1]\n",
    "                #test_batch_ += 1\n",
    "            #fig_loss2[tf.cast((i%100),dtype=tf.int32).eval()] = np.sum(test_loss_)/test_batch_\n",
    "            #fig_acc2[tf.cast((i%100),dtype=tf.int32).eval()] = np.sum(test_acc_)/test_batch_\n",
    "            \n",
    "            print('---------------------')\n",
    "            print('step:%d , test_error:%.6f' % (i, error[-1]))\n",
    "            print('---------------------')\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shishuhan\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "#sess.run(tf.local_variables_initializer())\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#step= sess.run([global_step], feed_dict={b:train_batch})\n",
    "sess = tf.InteractiveSession(config=tf_config)\n",
    "print(p.eval())\n",
    "#train_error = sess.run(error_op)\n",
    "#train_error1 = sess.run([error1_op], feed_dict={b:train_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_ = tf.random_normal(mean=0.0,stddev=tf.sqrt(0.5),shape=[2*K*M, N])\n",
    "pp = tf.complex(pp_[0:K*M,:], p_[K*M:,:])\n",
    "kk = tf.complex(tf.constant(1.0), tf.constant(0.0))\n",
    "def f():\n",
    "    global pp\n",
    "    for k in range(0,K):\n",
    "        part0 = pp[0:k*M,:]\n",
    "        #part1 = pp[k*M:(k+1)*M,:]\n",
    "        part2 = pp[(k+1)*M:,:]\n",
    "        part1 = tf.complex(tf.ones(shape=[M,1]), tf.zeros(shape=[M,1]))\n",
    "        pp = tf.concat([part0,part1,part2], axis=0)\n",
    "    return pp\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "print(sess.run(pp[0:M,:]))\n",
    "print(sess.run(f()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "kk1 = (b[:,0:(0+1)]/(2*tf.sqrt(2*3.14*rol))*tf.exp(-tf.complex(tf.pow(tf.real(b_hat[:,0:(0+1)]),2), tf.zeros(shape=[K,1]))/(2*tf.pow(rol,2))))\n",
    "#kk2 = tf.matmul(tf.transpose(tf.conj(H[0*L:(0+1)*L,:])), w[0*L:(0+1)*L,:])\n",
    "cof1 = tf.complex(tf.zeros(shape=[K,batch_size]), tf.zeros(shape=[K,batch_size]))\n",
    "part0 = cof1[:,:0]\n",
    "part1 = (b[:,0:(0+1)]/(2*tf.sqrt(2*3.14*rol))\n",
    "         *tf.exp(-tf.complex(tf.pow(tf.real(b_hat[:,0:(0+1)]),2), tf.zeros(shape=[K,1]))/(2*tf.pow(rol,2))))\n",
    "part2 = cof1[:,(0+1):]\n",
    "#cof1 = tf.concat([part0,part1,part2], axis=1)\n",
    "print(kk1)\n",
    "print(part0)\n",
    "print(part1)\n",
    "print(part2)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
